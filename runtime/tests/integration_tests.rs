//! Tests de integraci√≥n para async iterators
//!
//! Tests end-to-end que validan pipelines completos de procesamiento
//! de streams, escenarios del mundo real y interoperabilidad.

use std::collections::HashMap;
use std::sync::{Arc, Mutex, atomic::{AtomicUsize, Ordering}};
use std::time::{Duration, Instant};
use serde_json::json;
use vela_runtime::streams::{StreamBuilder, BackpressureController, BackpressureStrategy};
use vela_runtime::Stream;

/// Tests de pipelines completos
#[cfg(test)]
mod complete_pipeline_tests {
    use super::*;

    #[tokio::test]
    async fn test_basic_stream_processing() {
        // Test basic stream creation and subscription
        let data: Vec<i32> = (0..100).collect();
        let stream = StreamBuilder::from_iter(data.into_iter());

        let results: Arc<Mutex<Vec<i32>>> = Arc::new(Mutex::new(Vec::new()));
        let subscription = {
            let results_clone: Arc<Mutex<Vec<i32>>> = Arc::clone(&results);
            stream.subscribe(
                move |value| {
                    results_clone.lock().unwrap().push(value);
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(50)).await;

        let processed_results = results.lock().unwrap().clone();

        // Should have processed all values
        assert_eq!(processed_results.len(), 100);
        assert_eq!(processed_results, (0..100).collect::<Vec<_>>());

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_interval_stream_processing() {
        // Test interval stream with timing
        let stream = StreamBuilder::interval(Duration::from_millis(10));

        let counter: Arc<AtomicUsize> = Arc::new(AtomicUsize::new(0));
        let start_time = Instant::now();

        let subscription = {
            let counter_clone: Arc<AtomicUsize> = Arc::clone(&counter);
            stream.subscribe(
                move |_| { counter_clone.fetch_add(1, Ordering::SeqCst); },
                |_| {},
                || {},
            )
        };

        // Run for a reasonable time
        tokio::time::sleep(Duration::from_millis(600)).await;

        let elapsed = start_time.elapsed();
        let processed_count = counter.load(Ordering::SeqCst);

        println!("Interval stream processed {} values in {:?}", processed_count, elapsed);

        // Should have processed some values
        assert!(processed_count > 0);
        // Remove the upper bound check since we can't limit with .take()

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_multiple_subscriptions_same_stream() {
        // Test multiple subscriptions to streams with same data
        let data: Vec<i32> = (0..50).collect();

        let counters: Vec<Arc<AtomicUsize>> = (0..3)
            .map(|_| Arc::new(AtomicUsize::new(0)))
            .collect();

        let mut subscriptions = Vec::new();

        // Create 3 subscriptions (each with its own stream with same data)
        for i in 0..3 {
            let stream_data = data.clone();
            let stream = StreamBuilder::from_iter(stream_data.into_iter());
            let counter_clone: Arc<std::sync::atomic::AtomicUsize> = Arc::clone(&counters[i]);
            let subscription = stream.subscribe(
                move |_| { counter_clone.fetch_add(1, Ordering::SeqCst); },
                |_| {},
                || {},
            );
            subscriptions.push(subscription);
        }

        tokio::time::sleep(Duration::from_millis(100)).await;

        // All subscriptions should have received all values
        for (i, counter) in counters.iter().enumerate() {
            let count = counter.load(Ordering::SeqCst);
            println!("Subscription {} processed {} values", i, count);
            assert_eq!(count, 50);
        }

        // Clean up
        for subscription in subscriptions {
            subscription.unsubscribe();
        }
    }
}

/// Tests de escenarios del mundo real
#[cfg(test)]
mod real_world_scenarios {
    use super::*;

    #[tokio::test]
    async fn test_event_stream_processing() {
        // Simulate processing events from a stream
        let events = vec![
            "user_login", "page_view", "user_logout", "error_500",
            "user_login", "api_call", "user_logout", "page_view"
        ];

        let stream = StreamBuilder::from_iter(events.into_iter());

        let event_counts: Arc<Mutex<std::collections::HashMap<String, usize>>> = Arc::new(Mutex::new(std::collections::HashMap::new()));
        let subscription = {
            let counts_clone: Arc<Mutex<std::collections::HashMap<String, usize>>> = Arc::clone(&event_counts);
            stream.subscribe(
                move |event| {
                    let mut counts = counts_clone.lock().unwrap();
                    *counts.entry(event.to_string()).or_insert(0) += 1;
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(50)).await;

        let final_counts = event_counts.lock().unwrap().clone();

        // Verify event counts
        assert_eq!(*final_counts.get("user_login").unwrap_or(&0), 2);
        assert_eq!(*final_counts.get("page_view").unwrap_or(&0), 2);
        assert_eq!(*final_counts.get("user_logout").unwrap_or(&0), 2);
        assert_eq!(*final_counts.get("error_500").unwrap_or(&0), 1);
        assert_eq!(*final_counts.get("api_call").unwrap_or(&0), 1);

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_monitoring_data_collection() {
        // Simulate collecting monitoring data
        let stream = StreamBuilder::interval(Duration::from_millis(5));

        let metrics: Arc<Mutex<Vec<(u64, f64, u64)>>> = Arc::new(Mutex::new(Vec::new()));
        let start_time = Instant::now();

        let subscription = {
            let metrics_clone: Arc<Mutex<Vec<(u64, f64, u64)>>> = Arc::clone(&metrics);
            stream.subscribe(
                move |value| {
                    let timestamp = start_time.elapsed().as_millis() as u64;
                    let cpu_usage = (value % 100) as f64;
                    let memory_usage = 1000 + (value as u64 * 10);

                    metrics_clone.lock().unwrap().push((timestamp, cpu_usage, memory_usage));
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(600)).await;

        let collected_metrics = metrics.lock().unwrap().clone();

        println!("Collected {} monitoring data points", collected_metrics.len());

        // Should have collected some data points
        assert!(collected_metrics.len() > 50);

        // Verify data structure
        for (timestamp, cpu, memory) in &collected_metrics {
            assert!(*timestamp > 0);
            assert!(*cpu >= 0.0 && *cpu < 100.0);
            assert!(*memory >= 1000);
        }

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_log_processing_simulation() {
        // Simulate processing log entries
        let log_levels = vec!["INFO", "WARN", "ERROR", "DEBUG"];
        let logs: Vec<String> = (0..200)
            .map(|i| format!("{}: Message {}", log_levels[i % 4], i))
            .collect();

        let stream = StreamBuilder::from_iter(logs.into_iter());

        let processed_logs: Arc<Mutex<Vec<String>>> = Arc::new(Mutex::new(Vec::new()));
        let error_count: Arc<AtomicUsize> = Arc::new(AtomicUsize::new(0));

        let subscription = {
            let logs_clone: Arc<Mutex<Vec<String>>> = Arc::clone(&processed_logs);
            let error_clone: Arc<AtomicUsize> = Arc::clone(&error_count);
            stream.subscribe(
                move |log| {
                    logs_clone.lock().unwrap().push(log.clone());

                    // Count ERROR logs
                    if log.contains("ERROR") {
                        error_clone.fetch_add(1, Ordering::SeqCst);
                    }
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(100)).await;

        let final_logs = processed_logs.lock().unwrap().clone();
        let errors = error_count.load(Ordering::SeqCst);

        println!("Processed {} logs with {} errors", final_logs.len(), errors);

        // Should have processed all logs
        assert_eq!(final_logs.len(), 200);

        // Should have counted errors correctly (every 4th log is ERROR)
        assert_eq!(errors, 50);

        subscription.unsubscribe();
    }
}

/// Tests de interoperabilidad
#[cfg(test)]
mod interoperability_tests {
    use super::*;

    #[tokio::test]
    async fn test_stream_to_channel_conversion() {
        let (tx, mut rx) = tokio::sync::mpsc::channel(100);

        let data: Vec<i32> = (0..20).collect();
        let stream = StreamBuilder::from_iter(data.into_iter());

        // Convert stream to channel
        let subscription = stream.subscribe(
            move |value| {
                let tx_clone = tx.clone();
                tokio::spawn(async move {
                    let _ = tx_clone.send(value).await;
                });
            },
            |_| {},
            || {},
        );

        // Collect results from channel
        let mut results = Vec::new();
        let mut timeout_count = 0;

        while results.len() < 20 && timeout_count < 50 {
            match tokio::time::timeout(Duration::from_millis(10), rx.recv()).await {
                Ok(Some(value)) => results.push(value),
                Ok(None) => break,
                Err(_) => timeout_count += 1,
            }
        }

        println!("Collected {} values from channel", results.len());

        // Should have collected all values
        assert_eq!(results.len(), 20);
        assert_eq!(results, (0..20).collect::<Vec<_>>());

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_multiple_stream_composition() {
        // Create multiple streams and combine their results manually
        let data1: Vec<i32> = (0..10).collect();
        let data2: Vec<i32> = (10..20).collect();

        let stream1 = StreamBuilder::from_iter(data1.into_iter());
        let stream2 = StreamBuilder::from_iter(data2.into_iter());

        // Combine results manually
        let combined_results: Arc<Mutex<Vec<String>>> = Arc::new(Mutex::new(Vec::new()));

        let subscription1 = {
            let results_clone: Arc<Mutex<Vec<String>>> = Arc::clone(&combined_results);
            stream1.subscribe(
                move |value| results_clone.lock().unwrap().push(format!("stream1: {}", value)),
                |_| {},
                || {},
            )
        };

        let subscription2 = {
            let results_clone: Arc<Mutex<Vec<String>>> = Arc::clone(&combined_results);
            stream2.subscribe(
                move |value| results_clone.lock().unwrap().push(format!("stream2: {}", value)),
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(100)).await;

        let results = combined_results.lock().unwrap().clone();
        println!("Combined {} values from multiple streams", results.len());

        // Should have values from both streams
        assert_eq!(results.len(), 20);

        // Check that we have values from both streams
        let stream1_count = results.iter().filter(|s| s.starts_with("stream1:")).count();
        let stream2_count = results.iter().filter(|s| s.starts_with("stream2:")).count();

        assert_eq!(stream1_count, 10);
        assert_eq!(stream2_count, 10);

        subscription1.unsubscribe();
        subscription2.unsubscribe();
    }

    #[tokio::test]
    async fn test_error_handling_integration() {
        // Test error handling in subscription callbacks
        let data: Vec<Result<i32, String>> = vec![
            Ok(1), Ok(2), Err("test error".to_string()), Ok(4), Ok(5)
        ];

        let stream = StreamBuilder::from_iter(data.into_iter());

        let results = Arc::new(Mutex::new(Vec::new()));
        let errors = Arc::new(Mutex::new(Vec::<String>::new()));
        let completed = Arc::new(Mutex::new(false));

        let results_clone = Arc::clone(&results);
        let errors_clone = Arc::clone(&errors);
        let completed_clone = Arc::clone(&completed);

        let subscription = stream.subscribe(
            move |value| results_clone.lock().unwrap().push(value),
            move |error: Box<dyn std::error::Error>| errors_clone.lock().unwrap().push(error.to_string()),
            move || { *completed_clone.lock().unwrap() = true; },
        );

        tokio::time::sleep(Duration::from_millis(50)).await;

        let final_results = results.lock().unwrap().clone();
        let final_errors = errors.lock().unwrap().clone();
        let is_completed = *completed.lock().unwrap();

        println!("Got {} results and {} errors, completed: {}", final_results.len(), final_errors.len(), is_completed);

        // Should have processed successful values
        assert_eq!(final_results, vec![Ok(1), Ok(2), Err("test error".to_string()), Ok(4), Ok(5)]);

        // Should have captured errors (if error callback is invoked for Err values)
        // Note: This depends on how the stream handles Result values

        // Stream should complete
        assert!(is_completed);

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_backpressure_controller_integration() {
        // Test integration with BackpressureController
        let data: Vec<i32> = (0..1000).collect();
        let stream = StreamBuilder::from_iter(data.into_iter());

        // Create a backpressure controller
        let controller = Arc::new(BackpressureController::new(BackpressureStrategy::DropOldest, 10));

        let processed_count: Arc<AtomicUsize> = Arc::new(AtomicUsize::new(0));
        let subscription = {
            let counter_clone: Arc<AtomicUsize> = Arc::clone(&processed_count);
            let controller_clone: Arc<BackpressureController> = Arc::clone(&controller);
            stream.subscribe(
                move |value| {
                    // Simulate backpressure by occasionally delaying
                    if value % 100 == 0 {
                        std::thread::sleep(Duration::from_millis(1));
                    }
                    counter_clone.fetch_add(1, Ordering::SeqCst);
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(200)).await;

        let count = processed_count.load(Ordering::SeqCst);
        println!("Processed {} values with backpressure controller", count);

        // Should have processed many values
        assert!(count > 500);

        subscription.unsubscribe();
    }
}

/// Tests de performance end-to-end
#[cfg(test)]
mod performance_integration_tests {
    use super::*;

    #[tokio::test]
    async fn test_high_throughput_processing() {
        let start = Instant::now();
        let large_data: Vec<i32> = (0..10000).collect();
        let stream = StreamBuilder::from_iter(large_data.into_iter());

        let counter = Arc::new(AtomicUsize::new(0));
        let subscription = {
            let counter_clone = Arc::clone(&counter);
            stream.subscribe(
                move |_| { counter_clone.fetch_add(1, Ordering::SeqCst); },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_secs(2)).await;

        let duration = start.elapsed();
        let processed_count = counter.load(Ordering::SeqCst);

        println!("High throughput: processed {} values in {:?}", processed_count, duration);

        // Should have processed all values efficiently
        assert_eq!(processed_count, 10000);

        // Performance check: should process at least 1000 values per second
        let throughput = processed_count as f64 / duration.as_secs_f64();
        assert!(throughput > 1000.0);

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_memory_usage_stability() {
        // Test that memory usage remains stable over time
        let stream = StreamBuilder::interval(Duration::from_millis(1));

        let counter = Arc::new(AtomicUsize::new(0));
        let subscription = {
            let counter_clone = Arc::clone(&counter);
            stream.subscribe(
                move |_| { counter_clone.fetch_add(1, Ordering::SeqCst); },
                |_| {},
                || {},
            )
        };

        // Run for an extended period
        tokio::time::sleep(Duration::from_secs(5)).await;

        let processed_count = counter.load(Ordering::SeqCst);
        println!("Memory stability test processed {} values", processed_count);

        // Should have processed many values without memory issues
        assert!(processed_count > 4000);

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_concurrent_stream_operations() {
        // Test multiple streams running concurrently
        let mut subscriptions = Vec::new();
        let total_processed = Arc::new(AtomicUsize::new(0));

        // Create 10 concurrent streams
        for i in 0..10 {
            let data: Vec<i32> = ((i * 100)..((i + 1) * 100)).collect();
            let stream = StreamBuilder::from_iter(data.into_iter());

            let total_clone = Arc::clone(&total_processed);
            let subscription = stream.subscribe(
                move |_| { total_clone.fetch_add(1, Ordering::SeqCst); },
                |_| {},
                || {},
            );
            subscriptions.push(subscription);
        }

        tokio::time::sleep(Duration::from_millis(500)).await;

        let total_count = total_processed.load(Ordering::SeqCst);
        println!("Concurrent streams processed {} total values", total_count);

        // Should have processed all values from all streams
        assert_eq!(total_count, 1000);

        // Clean up
        for subscription in subscriptions {
            subscription.unsubscribe();
        }
    }

    #[tokio::test]
    async fn test_data_processing_pipeline() {
        // Simulate a complete data processing pipeline
        let raw_data = vec![
            json!({"id": 1, "value": 10, "category": "A"}),
            json!({"id": 2, "value": 25, "category": "B"}),
            json!({"id": 3, "value": 5, "category": "A"}),
            json!({"id": 4, "value": 30, "category": "C"}),
            json!({"id": 5, "value": 15, "category": "B"}),
        ];

        let stream = StreamBuilder::from_iter(raw_data.into_iter());

        let results = Arc::new(Mutex::new(Vec::new()));
        let subscription = {
            let results_clone = Arc::clone(&results);
            stream.subscribe(
                move |item| {
                    // Parse and validate data
                    let id = item["id"].as_i64().unwrap();
                    let value = item["value"].as_i64().unwrap();
                    let category = item["category"].as_str().unwrap().to_string();

                    // Filter high-value items (value > 10)
                    if value > 10 {
                        let mut groups: HashMap<String, Vec<(i64, i64)>> = HashMap::new();
                        groups.entry(category).or_insert(Vec::new()).push((id, value));
                        results_clone.lock().unwrap().push(groups);
                    }
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(100)).await;

        let processed_results = results.lock().unwrap().clone();

        // Should have processed the grouped data
        assert!(!processed_results.is_empty());

        // Verify grouping logic
        for result in processed_results {
            for (category, items) in result {
                // All items in category should have value > 10
                for (_, value) in &items {
                    assert!(*value > 10);
                }
                println!("Category {}: {} items", category, items.len());
            }
        }

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_realtime_monitoring_pipeline() {
        // Simulate real-time system monitoring
        let metrics_stream = StreamBuilder::interval(Duration::from_millis(10));

        let alerts = Arc::new(Mutex::new(Vec::new()));
        let subscription = {
            let alerts_clone = Arc::clone(&alerts);
            metrics_stream.subscribe(
                move |i| {
                    // Simulate CPU usage metrics
                    let cpu_usage = (i % 100) as f64;
                    let memory_usage = 1024.0 + (i as f64 * 10.0);
                    let timestamp = Instant::now();

                    // Alert on high CPU usage (simplified logic)
                    if cpu_usage > 50.0 {
                        alerts_clone.lock().unwrap().push((cpu_usage, memory_usage, timestamp));
                    }
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(1500)).await;

        let alert_list = alerts.lock().unwrap().clone();
        println!("Generated {} alerts", alert_list.len());

        // Should have generated some alerts for high CPU periods
        assert!(!alert_list.is_empty());

        // Verify alert conditions
        for (cpu, _, _) in &alert_list {
            assert!(*cpu > 50.0);
        }

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_event_processing_pipeline() {
        // Simulate event processing with complex transformations
        let events = vec![
            Event::UserAction { user_id: 1, action: "login".to_string(), timestamp: 1000 },
            Event::SystemEvent { event_type: "startup".to_string(), severity: 1, timestamp: 1001 },
            Event::UserAction { user_id: 2, action: "logout".to_string(), timestamp: 1002 },
            Event::ErrorEvent { error_code: 500, message: "Server error".to_string(), timestamp: 1003 },
            Event::UserAction { user_id: 1, action: "view_page".to_string(), timestamp: 1004 },
        ];

        let stream = StreamBuilder::from_iter(events.into_iter());

        let results = Arc::new(Mutex::new(Vec::new()));
        let subscription = {
            let results_clone = Arc::clone(&results);
            stream.subscribe(
                move |event| {
                    let processed_at = Instant::now();
                    let event_type = match &event {
                        Event::UserAction { .. } => "user_action",
                        Event::SystemEvent { .. } => "system_event",
                        Event::ErrorEvent { .. } => "error",
                    };

                    // Filter and route different event types (skip system events)
                    if event_type != "system_event" {
                        let mut grouped: HashMap<String, Vec<Event>> = HashMap::new();

                        // Apply different processing per event type
                        match &event {
                            Event::UserAction { action, .. } => {
                                if action != "logout" {
                                    grouped.entry(event_type.to_string()).or_insert(Vec::new()).push(event);
                                }
                            }
                            Event::ErrorEvent { mut error_code, .. } => {
                                // Simulate error severity escalation
                                let escalated_code = error_code + 1000;
                                let escalated_event = Event::ErrorEvent {
                                    error_code: escalated_code,
                                    message: "Server error".to_string(),
                                    timestamp: 1003
                                };
                                grouped.entry(event_type.to_string()).or_insert(Vec::new()).push(escalated_event);
                            }
                            _ => {}
                        }

                        if !grouped.is_empty() {
                            results_clone.lock().unwrap().push(grouped);
                        }
                    }
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(100)).await;

        let processed_results = results.lock().unwrap().clone();
        assert!(!processed_results.is_empty());

        // Verify processing logic
        for result in processed_results {
            // Check user actions were filtered
            if let Some(user_actions) = result.get("user_action") {
                for action in user_actions {
                    if let Event::UserAction { action: act, .. } = action {
                        assert_ne!(act, "logout");
                    }
                }
            }

            // Check error codes were escalated
            if let Some(errors) = result.get("error") {
                for error in errors {
                    if let Event::ErrorEvent { error_code, .. } = error {
                        assert!(*error_code >= 1500); // 500 + 1000
                    }
                }
            }
        }

        subscription.unsubscribe();
    }
}

/// Tests de escenarios del mundo real (actualizados)
#[cfg(test)]
mod real_world_scenarios_updated {
    use super::*;

    #[tokio::test]
    async fn test_api_rate_limiting_simulation() {
        // Simulate API requests with rate limiting
        let requests = (0..200).map(|i| {
            APIRequest {
                id: i,
                endpoint: format!("/api/v1/resource/{}", i % 10),
                method: if i % 3 == 0 { "POST" } else { "GET" }.to_string(),
                timestamp: Instant::now() + Duration::from_millis(i as u64 * 10),
            }
        }).collect::<Vec<_>>();

        let stream = StreamBuilder::from_iter(requests.into_iter());

        let processed_batches = Arc::new(AtomicUsize::new(0));
        let subscription = {
            let counter_clone = Arc::clone(&processed_batches);
            stream.subscribe(
                move |_| { counter_clone.fetch_add(1, Ordering::SeqCst); },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_secs(5)).await;

        let batches_processed = processed_batches.load(Ordering::SeqCst);
        println!("Processed {} batches of API requests", batches_processed);

        // Should have processed multiple batches
        assert!(batches_processed > 10);

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_log_aggregation_system() {
        // Simulate distributed log aggregation (simplified for basic API)
        let logs = (0..500).map(|i| {
            let level = match i % 4 {
                0 => "INFO",
                1 => "WARN",
                2 => "ERROR",
                _ => "DEBUG",
            };
            let service = format!("service-{}", i % 5);
            let message = format!("Log message {}", i);

            LogEntry {
                timestamp: Instant::now(),
                level: level.to_string(),
                service,
                message,
                trace_id: Some(format!("trace-{}", i / 10)),
            }
        }).collect::<Vec<_>>();

        // Filter logs manually (no .filter() operator available)
        let filtered_logs: Vec<LogEntry> = logs.into_iter()
            .filter(|log| log.level != "DEBUG")
            .collect();

        let stream = StreamBuilder::from_iter(filtered_logs.into_iter());

        let processed_logs = Arc::new(AtomicUsize::new(0));
        let error_logs = Arc::new(AtomicUsize::new(0));

        let subscription = {
            let processed_clone = Arc::clone(&processed_logs);
            let error_clone = Arc::clone(&error_logs);
            stream.subscribe(
                move |log: LogEntry| {
                    processed_clone.fetch_add(1, Ordering::SeqCst);
                    if log.level == "ERROR" {
                        error_clone.fetch_add(1, Ordering::SeqCst);
                    }
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(100)).await;

        let total_processed = processed_logs.load(Ordering::SeqCst);
        let total_errors = error_logs.load(Ordering::SeqCst);

        println!("Processed {} logs, {} were errors", total_processed, total_errors);

        // Should have processed most non-debug logs
        assert!(total_processed > 350); // ~375 non-debug logs expected
        assert!(total_errors > 100); // ~125 error logs expected

        subscription.unsubscribe();
    }

    #[tokio::test]
    async fn test_financial_transaction_processing() {
        // Simulate financial transaction processing with fraud detection (simplified)
        let transactions = (0..1000).map(|i| {
            let amount = (i as f64 * 10.5) % 10000.0;
            let account_from = format!("acc-{:04}", i % 100);
            let account_to = format!("acc-{:04}", (i + 1) % 100);
            let timestamp = Instant::now();

            Transaction {
                id: format!("tx-{}", i),
                amount,
                account_from,
                account_to,
                timestamp,
                suspicious: amount > 5000.0, // Flag large transactions
            }
        }).collect::<Vec<_>>();

        // Process transactions manually (no operators available)
        let processed_transactions: Vec<(Transaction, f64)> = transactions.into_iter()
            .map(|tx| {
                let fraud_score = if tx.amount > 5000.0 {
                    0.8
                } else if tx.amount > 1000.0 {
                    0.3
                } else {
                    0.1
                };
                (tx, fraud_score)
            })
            .collect();

        let suspicious_transactions: Vec<(Transaction, f64)> = processed_transactions.into_iter()
            .filter(|(_, fraud_score)| *fraud_score > 0.5)
            .collect();

        let stream = StreamBuilder::from_iter(suspicious_transactions.into_iter());

        let fraud_alerts = Arc::new(AtomicUsize::new(0));
        let suspicious_count = Arc::new(AtomicUsize::new(0));

        let subscription = {
            let alerts_clone = Arc::clone(&fraud_alerts);
            let count_clone = Arc::clone(&suspicious_count);
            stream.subscribe(
                move |(tx, score)| {
                    count_clone.fetch_add(1, Ordering::SeqCst);
                    if score > 0.7 {
                        alerts_clone.fetch_add(1, Ordering::SeqCst);
                    }
                    println!("Suspicious transaction: {} (${:.2})", tx.id, tx.amount);
                },
                |_| {},
                || {},
            )
        };

        tokio::time::sleep(Duration::from_millis(100)).await;

        let alerts_count = fraud_alerts.load(Ordering::SeqCst);
        let total_suspicious = suspicious_count.load(Ordering::SeqCst);

        println!("Processed {} suspicious transactions, {} high-risk alerts",
                total_suspicious, alerts_count);

        // Should have detected some suspicious transactions
        assert!(total_suspicious > 300); // ~400 transactions with score > 0.5
        assert!(alerts_count > 100); // ~200 transactions with score > 0.7
        assert!(alerts_count > 0);

        subscription.unsubscribe();
    }
}

// Mock data structures for tests
#[derive(Clone, Debug)]
struct APIRequest {
    id: usize,
    endpoint: String,
    method: String,
    timestamp: Instant,
}

#[derive(Clone, Debug)]
struct LogEntry {
    timestamp: Instant,
    level: String,
    service: String,
    message: String,
    trace_id: Option<String>,
}

#[derive(Clone, Debug)]
struct Transaction {
    id: String,
    amount: f64,
    account_from: String,
    account_to: String,
    timestamp: Instant,
    suspicious: bool,
}

#[derive(Clone, Debug)]
enum Event {
    UserAction { user_id: i64, action: String, timestamp: i64 },
    SystemEvent { event_type: String, severity: i32, timestamp: i64 },
    ErrorEvent { error_code: i32, message: String, timestamp: i64 },
}