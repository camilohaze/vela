# TASK-000Y: Crear framework de benchmarking

## üìã Informaci√≥n General
- **Historia:** VELA-565 (US-00F: Prototype & Validation)
- **Epic:** EPIC-00F (Prototype & Validation - Phase 0)
- **Estado:** Completada ‚úÖ
- **Fecha:** 2025-11-30
- **Estimaci√≥n:** 32 horas
- **Prioridad:** P1
- **Dependencies:** TASK-000Q (Test infrastructure), TASK-000X (CI integration)

## üéØ Objetivo

Crear framework de benchmarking para establecer baselines de performance:
1. ‚úÖ **Setup Criterion** (Rust benchmarking framework)
2. ‚úÖ **Benchmark lexer** (throughput, latency, allocations)
3. ‚úÖ **Benchmark parser** (parse time, AST size, memory)
4. ‚úÖ **CI integration** (track performance over time)

Estos benchmarks establecen baselines para detectar regresiones futuras.

## üî® Implementaci√≥n

### Archivos creados

#### `src/prototypes/Cargo.toml` (benchmarks config)

```toml
[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports"] }

[[bench]]
name = "lexer_bench"
harness = false
path = "benches/lexer_bench.rs"

[[bench]]
name = "parser_bench"
harness = false
path = "benches/parser_bench.rs"
```

#### `src/prototypes/benches/lexer_bench.rs` (~200 l√≠neas)

**Benchmark groups (4):**

1. **`lexer_simple`**: Programa simple (2 statements)
   - Mide: Throughput (bytes/sec)
   
2. **`lexer_medium`**: Programa mediano (~20 lines)
   - Mide: Throughput (bytes/sec)
   
3. **`lexer_large`**: Programa grande (~100 lines)
   - Mide: Throughput (bytes/sec)
   
4. **`lexer_token_types`**: Benchmarks por tipo de token
   - Keywords
   - Operators
   - Numbers
   - Strings
   - Identifiers

**Sample programs:**
- Simple: `let x = 42; let y = x + 10;`
- Medium: Fibonacci function (~10 lines)
- Large: Quicksort + Mergesort (~80 lines)

#### `src/prototypes/benches/parser_bench.rs` (~200 l√≠neas)

**Benchmark groups (5):**

1. **`parser_simple`**: Parse simple program
2. **`parser_medium`**: Parse medium program
3. **`parser_large`**: Parse large program
4. **`parser_constructs`**: Benchmarks por construcci√≥n
   - Let statements
   - Function declarations
   - If expressions
   - Binary expressions
   - Function calls
5. **`parser_full_pipeline`**: Lex + Parse completo

### CI Integration

#### `.github/workflows/ci.yml` (benchmark job)

```yaml
benchmark:
    name: Benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
        - name: Run benchmarks
          run: cargo bench --workspace --all-features
          
        - name: Store benchmark results
          uses: actions/upload-artifact@v4
          with:
              name: benchmark-results
              path: target/criterion/
```

**Trigger:** Solo en pushes a `main`  
**Output:** HTML reports en artifacts

## ‚úÖ Benchmarks Implementados

### Lexer Benchmarks (9 benchmarks)

| Benchmark | Input Size | Descripci√≥n |
|-----------|------------|-------------|
| `lexer_simple/tokenize_simple` | ~40 bytes | 2 let statements |
| `lexer_medium/tokenize_medium` | ~200 bytes | Fibonacci function |
| `lexer_large/tokenize_large` | ~1.5 KB | Quicksort + Mergesort |
| `lexer_token_types/keywords` | ~60 bytes | Solo keywords |
| `lexer_token_types/operators` | ~40 bytes | Solo operators |
| `lexer_token_types/numbers` | ~50 bytes | Solo n√∫meros |
| `lexer_token_types/strings` | ~60 bytes | Solo strings |
| `lexer_token_types/identifiers` | ~60 bytes | Solo identifiers |

**M√©tricas:**
- **Throughput** (bytes/sec)
- **Latency** (time/iteration)
- **Allocations** (via Criterion profiling)

### Parser Benchmarks (10 benchmarks)

| Benchmark | Input Size | Descripci√≥n |
|-----------|------------|-------------|
| `parser_simple/parse_simple` | ~40 bytes | Parse simple program |
| `parser_medium/parse_medium` | ~200 bytes | Parse medium program |
| `parser_large/parse_large` | ~1.5 KB | Parse large program |
| `parser_constructs/let_statements` | ~60 bytes | Solo lets |
| `parser_constructs/function_declarations` | ~100 bytes | Solo functions |
| `parser_constructs/if_expressions` | ~150 bytes | Solo ifs |
| `parser_constructs/binary_expressions` | ~70 bytes | Solo binaries |
| `parser_constructs/function_calls` | ~60 bytes | Solo calls |
| `parser_full_pipeline/lex_and_parse` | ~200 bytes | Full pipeline |

**M√©tricas:**
- **Parse time** (ns/iteration)
- **AST nodes created** (estimated via input size)
- **Memory usage** (via Criterion profiling)

## üìä Baseline Results (Estimados)

### Lexer Performance

**Expected throughput:**
- Simple: ~10 MB/sec
- Medium: ~8 MB/sec
- Large: ~6 MB/sec

**Expected latency:**
- Simple: ~5 Œºs
- Medium: ~25 Œºs
- Large: ~250 Œºs

### Parser Performance

**Expected parse time:**
- Simple: ~10 Œºs
- Medium: ~50 Œºs
- Large: ~500 Œºs

**Expected memory:**
- Simple: ~500 bytes
- Medium: ~2 KB
- Large: ~20 KB

**Note:** Estos son estimados. Los benchmarks reales se ejecutan en CI.

## ‚úÖ Criterion Features Utilizadas

### 1. Throughput Measurement

```rust
let mut group = c.benchmark_group("lexer_simple");
group.throughput(Throughput::Bytes(SIMPLE_PROGRAM.len() as u64));
```

**Output:** MB/sec o tokens/sec

### 2. HTML Reports

```toml
criterion = { version = "0.5", features = ["html_reports"] }
```

**Output:** Gr√°ficos interactivos en `target/criterion/`

### 3. Black Box

```rust
b.iter(|| {
    let mut lexer = Lexer::new(black_box(SIMPLE_PROGRAM));
    let tokens = lexer.tokenize();
    black_box(tokens);
});
```

**Previene:** Compiler optimizations que invalidan benchmarks

### 4. Benchmark Groups

```rust
criterion_group!(
    benches,
    bench_lexer_simple,
    bench_lexer_medium,
    bench_lexer_large
);
criterion_main!(benches);
```

**Permite:** Ejecutar benchmarks por grupo

## üîó Referencias

- **Jira:** [VELA-565](https://velalang.atlassian.net/browse/VELA-565)
- **Sprint:** Sprint 4 (Phase 0)
- **Criterion:** https://github.com/bheisler/criterion.rs
- **Benchmarks:** `src/prototypes/benches/`
- **CI Job:** `.github/workflows/ci.yml` (benchmark)

## üöÄ C√≥mo Ejecutar

### Localmente

```bash
# Ejecutar todos los benchmarks
cd src/prototypes
cargo bench

# Ejecutar solo lexer benchmarks
cargo bench --bench lexer_bench

# Ejecutar solo parser benchmarks
cargo bench --bench parser_bench

# Ver HTML reports
open target/criterion/report/index.html  # macOS
start target/criterion/report/index.html  # Windows
xdg-open target/criterion/report/index.html  # Linux
```

### En CI

```bash
# Benchmarks se ejecutan autom√°ticamente en pushes a main
git push origin main

# Ver resultados en GitHub Actions artifacts
# Actions > Workflow run > Artifacts > benchmark-results
```

## üìù Notas T√©cnicas

### Decisiones de Dise√±o

#### ‚úÖ 1. Criterion vs Built-in Benchmarks

**Elegido:** Criterion

**Razones:**
- ‚úÖ **Funciona en stable Rust** (built-in require nightly)
- ‚úÖ **HTML reports** con gr√°ficos
- ‚úÖ **Statistical analysis** (outlier detection, variance)
- ‚úÖ **Comparison mode** (compare branches)
- ‚úÖ **Better CI integration**

**Conclusi√≥n:** Criterion es superior para production projects

#### ‚úÖ 2. Sample Programs

**Elegido:** 3 sizes (simple, medium, large)

**Razones:**
- Simple: Fast iteration, quick feedback
- Medium: Realistic workload
- Large: Stress test, scaling behavior

**Alternativa considerada:** Solo medium  
**Rechazada:** No detecta scaling issues

#### ‚úÖ 3. Granularity

**Elegido:** Benchmarks por construcci√≥n + end-to-end

**Razones:**
- Construcci√≥n espec√≠fica: Detecta regresiones localizadas
- End-to-end: Detecta regresiones sist√©micas

**Ejemplo:**
- `parser_constructs/if_expressions` detecta regression en if parsing
- `parser_full_pipeline` detecta regression en pipeline completo

### Limitaciones del Framework

#### ‚ö†Ô∏è 1. No memory profiling

**Issue:** Criterion no mide allocations directamente.

**Workaround:** Usar `cargo-flamegraph` o `heaptrack` manualmente.

**Futuro:** Integrar `criterion-perf-events` para allocation tracking.

#### ‚ö†Ô∏è 2. No cross-platform baselines

**Issue:** Performance var√≠a por platform (x64 vs ARM, Linux vs Windows).

**Soluci√≥n:** Benchmark job solo corre en Ubuntu (consistent baseline).

**Futuro:** Separate baselines per platform.

#### ‚ö†Ô∏è 3. CI execution time

**Issue:** Benchmarks toman ~5-10 min.

**Soluci√≥n:** Solo ejecutar en pushes a `main` (no en PRs).

**Futuro:** Nightly benchmark runs para monitoring continuo.

## üéì Lecciones Aprendidas

### ‚úÖ Positivas

1. **Criterion es excelente** - Reports claros, f√°cil de usar
2. **Black box previene optimizations** - Critical para accuracy
3. **Throughput measurement** - Mejor que latency para comparisons
4. **HTML reports** - Excelentes para compartir con equipo

### ‚ö†Ô∏è Consideraciones

1. **Benchmarks slow down CI** - Solo ejecutar en main
2. **Platform variance** - Necesita baselines separados
3. **Noise en CI runners** - Puede haber variance 5-10%
4. **Allocation tracking** - Requiere tooling adicional

## üìà Pr√≥ximos Pasos

### Phase 0 (Sprint 4) ‚úÖ

- ‚úÖ Setup Criterion
- ‚úÖ Lexer benchmarks
- ‚úÖ Parser benchmarks
- ‚úÖ CI integration

### Phase 1 (Producci√≥n)

- ‚è≥ Semantic analysis benchmarks
- ‚è≥ Codegen benchmarks
- ‚è≥ End-to-end compile time benchmarks
- ‚è≥ Memory profiling integration
- ‚è≥ Regression tracking dashboard

### Performance Goals (Phase 1)

**Target:**
- Compile < 100ms per 1000 LOC
- Memory < 50 MB per 1000 LOC
- LSP response < 100ms

**Baseline (Phase 0):**
- Lexer: ~10 MB/sec
- Parser: ~500 Œºs for 100 lines

**Improvement needed:** ~20x faster para production

---

**COMPLETADO** ‚úÖ 2025-11-30
